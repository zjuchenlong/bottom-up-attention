{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _init_paths\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import caffe\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Setting\n",
    "coco_img_path = '/home/cl/Dataset/COCO/'\n",
    "coco_anno_path = '/home/cl/Dataset/COCO/annotations'\n",
    "\n",
    "# Caffe Setting\n",
    "gpu_id = 1\n",
    "caffe_proto = '../models/vg/ResNet-101/faster_rcnn_end2end_final/test.prototxt'\n",
    "caffe_model = '../data/faster_rcnn_models/resnet101_faster_rcnn_final_iter_320000.caffemodel'\n",
    "\n",
    "# Faster-RCNN Setting\n",
    "PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n",
    "TEST_SCALES = [600]\n",
    "MAX_SIZE = 1000\n",
    "HAS_RPN = True # True means no extra bounding box, False means with extra bounding box input\n",
    "DEDUP_BOXES = 0.0625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_train_anno_path = os.path.join(coco_anno_path, 'captions_train2014.json')\n",
    "coco_val_anno_path = os.path.join(coco_anno_path, 'captions_val2014.json')\n",
    "coco_train_img_path = os.path.join(coco_img_path, 'train2014')\n",
    "coco_val_img_path = os.path.join(coco_img_path, 'val2014')\n",
    "\n",
    "def assert_exist_path(path):\n",
    "    assert os.path.exists(path)\n",
    "\n",
    "assert_exist_path(caffe_proto)\n",
    "assert_exist_path(caffe_model)\n",
    "    \n",
    "assert_exist_path(coco_train_anno_path)\n",
    "assert_exist_path(coco_val_anno_path)\n",
    "assert_exist_path(coco_train_img_path)\n",
    "assert_exist_path(coco_val_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82783/82783 [00:00<00:00, 178436.67it/s]\n",
      "100%|██████████| 40504/40504 [00:00<00:00, 153878.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_image_info(split_name):\n",
    "    split = []\n",
    "    if split_name == 'train':\n",
    "        with open(coco_train_anno_path) as f:\n",
    "            data  = json.load(f)\n",
    "            for item in tqdm(data['images']):\n",
    "                image_id = int(item['id'])\n",
    "                image_width = int(item['width'])\n",
    "                image_height = int(item['height'])\n",
    "                image_path = os.path.join(coco_train_img_path, item['file_name'])\n",
    "                assert_exist_path(image_path)\n",
    "                split.append({'id': image_id, 'width': image_width, \n",
    "                             'height': image_height, 'filename': image_path})\n",
    "    elif split_name == 'val':\n",
    "        with open(coco_val_anno_path) as f:\n",
    "            data  = json.load(f)\n",
    "            for item in tqdm(data['images']):\n",
    "                image_id = int(item['id'])\n",
    "                image_width = int(item['width'])\n",
    "                image_height = int(item['height'])\n",
    "                image_path = os.path.join(coco_val_img_path, item['file_name'])\n",
    "                assert_exist_path(image_path)\n",
    "                split.append({'id': image_id, 'width': image_width, \n",
    "                             'height': image_height, 'filename': image_path})\n",
    "    else:\n",
    "        print 'Unknow split'\n",
    "    return split\n",
    "\n",
    "train_img_info = filter_image_info('train')\n",
    "val_img_info = filter_image_info('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/zjuchenlong/bottom-up-attention/blob/master/lib/utils/blob.py\n",
    "\n",
    "def im_list_to_blob(ims):\n",
    "    \"\"\"Convert a list of images into a network input.\n",
    "    Assumes images are already prepared (means subtracted, BGR order, ...).\n",
    "    \"\"\"\n",
    "    max_shape = np.array([im.shape for im in ims]).max(axis=0)\n",
    "    num_images = len(ims)\n",
    "    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n",
    "                    dtype=np.float32)\n",
    "    for i in xrange(num_images):\n",
    "        im = ims[i]\n",
    "        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n",
    "    # Move channels (axis 3) to axis 1\n",
    "    # Axis order will become: (batch elem, channel, height, width)\n",
    "    channel_swap = (0, 3, 1, 2)\n",
    "    blob = blob.transpose(channel_swap)\n",
    "    return blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/zjuchenlong/bottom-up-attention/blob/master/lib/fast_rcnn/test.py\n",
    "\n",
    "def _get_image_blob(im):\n",
    "    \"\"\"Converts an image into a network input.\n",
    "    Arguments:\n",
    "        im (ndarray): a color image in BGR order\n",
    "    Returns:\n",
    "        blob (ndarray): a data blob holding an image pyramid\n",
    "        im_scale_factors (list): list of image scales (relative to im) used\n",
    "            in the image pyramid\n",
    "    \"\"\"\n",
    "    im_orig = im.astype(np.float32, copy=True)\n",
    "    im_orig -= PIXEL_MEANS\n",
    "\n",
    "    im_shape = im_orig.shape\n",
    "    im_size_min = np.min(im_shape[0:2])\n",
    "    im_size_max = np.max(im_shape[0:2])\n",
    "\n",
    "    processed_ims = []\n",
    "    im_scale_factors = []\n",
    "\n",
    "    for target_size in TEST_SCALES:\n",
    "        im_scale = float(target_size) / float(im_size_min)\n",
    "        # Prevent the biggest axis from being more than MAX_SIZE\n",
    "        if np.round(im_scale * im_size_max) > MAX_SIZE:\n",
    "            im_scale = float(MAX_SIZE) / float(im_size_max)\n",
    "        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n",
    "                        interpolation=cv2.INTER_LINEAR)\n",
    "        im_scale_factors.append(im_scale)\n",
    "        processed_ims.append(im)\n",
    "\n",
    "    # Create a blob to hold the input images\n",
    "    blob = im_list_to_blob(processed_ims)\n",
    "\n",
    "    return blob, np.array(im_scale_factors)\n",
    "\n",
    "def _get_rois_blob(im_rois, im_scale_factors):\n",
    "    \"\"\"Converts RoIs into network inputs.\n",
    "    Arguments:\n",
    "        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n",
    "        im_scale_factors (list): scale factors as returned by _get_image_blob\n",
    "    Returns:\n",
    "        blob (ndarray): R x 5 matrix of RoIs in the image pyramid\n",
    "    \"\"\"\n",
    "    rois, levels = _project_im_rois(im_rois, im_scale_factors)\n",
    "    rois_blob = np.hstack((levels, rois))\n",
    "    return rois_blob.astype(np.float32, copy=False)\n",
    "\n",
    "def _project_im_rois(im_rois, scales):\n",
    "    \"\"\"Project image RoIs into the image pyramid built by _get_image_blob.\n",
    "    Arguments:\n",
    "        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n",
    "        scales (list): scale factors as returned by _get_image_blob\n",
    "    Returns:\n",
    "        rois (ndarray): R x 4 matrix of projected RoI coordinates\n",
    "        levels (list): image pyramid levels used by each projected RoI\n",
    "    \"\"\"\n",
    "    im_rois = im_rois.astype(np.float, copy=False)\n",
    "\n",
    "    if len(scales) > 1:\n",
    "        widths = im_rois[:, 2] - im_rois[:, 0] + 1\n",
    "        heights = im_rois[:, 3] - im_rois[:, 1] + 1\n",
    "\n",
    "        areas = widths * heights\n",
    "        scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n",
    "        diff_areas = np.abs(scaled_areas - 224 * 224)\n",
    "        levels = diff_areas.argmin(axis=1)[:, np.newaxis]\n",
    "    else:\n",
    "        levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n",
    "\n",
    "    rois = im_rois * scales[levels]\n",
    "\n",
    "    return rois, levels\n",
    "\n",
    "def _get_blobs(im, rois=None):\n",
    "    \"\"\"\n",
    "    Convert an image and RoIs within that image into network inputs.\n",
    "    boxes (ndarray): R x 4 array of object proposals\n",
    "    \"\"\"\n",
    "    blobs = {'data' : None, 'rois' : None}\n",
    "    blobs['data'], im_scale_factors = _get_image_blob(im)\n",
    "    if not HAS_RPN:\n",
    "        blobs['rois'] = _get_rois_blob(rois, im_scale_factors)\n",
    "    return blobs, im_scale_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/zjuchenlong/bottom-up-attention/blob/master/lib/fast_rcnn/bbox_transform.py\n",
    "\n",
    "def bbox_transform(ex_rois, gt_rois):\n",
    "    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n",
    "    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n",
    "    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n",
    "    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n",
    "\n",
    "    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n",
    "    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n",
    "    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n",
    "    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n",
    "\n",
    "    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n",
    "    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n",
    "    targets_dw = np.log(gt_widths / ex_widths)\n",
    "    targets_dh = np.log(gt_heights / ex_heights)\n",
    "\n",
    "    targets = np.vstack(\n",
    "        (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n",
    "    return targets\n",
    "\n",
    "def bbox_transform_inv(boxes, deltas):\n",
    "    if boxes.shape[0] == 0:\n",
    "        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n",
    "    boxes = boxes.astype(deltas.dtype, copy=False)\n",
    "\n",
    "    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n",
    "    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n",
    "    ctr_x = boxes[:, 0] + 0.5 * widths\n",
    "    ctr_y = boxes[:, 1] + 0.5 * heights\n",
    "\n",
    "    dx = deltas[:, 0::4]\n",
    "    dy = deltas[:, 1::4]\n",
    "    dw = deltas[:, 2::4]\n",
    "    dh = deltas[:, 3::4]\n",
    "\n",
    "    pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n",
    "    pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n",
    "    pred_w = np.exp(dw) * widths[:, np.newaxis]\n",
    "    pred_h = np.exp(dh) * heights[:, np.newaxis]\n",
    "\n",
    "    pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n",
    "    # x1\n",
    "    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n",
    "    # y1\n",
    "    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n",
    "    # x2\n",
    "    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n",
    "    # y2\n",
    "    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "    return pred_boxes\n",
    "\n",
    "def clip_boxes(boxes, im_shape):\n",
    "    \"\"\"\n",
    "    Clip boxes to image boundaries.\n",
    "    \"\"\"\n",
    "\n",
    "    # x1 >= 0\n",
    "    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n",
    "    # y1 >= 0\n",
    "    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n",
    "    # x2 < im_shape[1]\n",
    "    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n",
    "    # y2 < im_shape[0]\n",
    "    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(gpu_id)\n",
    "caffe_net = caffe.Net(caffe_proto, caffe.TEST, weights=caffe_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in train_img_info:\n",
    "    im = cv2.imread(img['filename'])\n",
    "    blobs, im_scales = _get_blobs(im)\n",
    "    \n",
    "    # When mapping from image ROIs to feature map ROIs, there's some aliasing (some distinct image ROIs get mapped \n",
    "    # to the same feature ROI). Here, we identify duplicate feature ROIs, so we only compute features on the unique subset.\n",
    "    if DEDUP_BOXES > 0 and not HAS_RPN:\n",
    "        v = np.array([1, 1e3, 1e6, 1e9, 1e12])\n",
    "        hashes = np.round(blobs['rois'] * DEDUP_BOXES).dot(v)\n",
    "        _, index, inv_index = np.unique(hashes, return_index=True, return_inverse=True)\n",
    "        blobs['rois'] = blobs['rois'][index, :]\n",
    "        boxes = boxes[index, :]\n",
    "    \n",
    "    im_blob = blobs['data']\n",
    "    blobs['im_info'] = np.array([[im_blob.shape[2], im_blob.shape[3], im_scales[0]]], dtype=np.float32)\n",
    "\n",
    "    # reshape network inputs\n",
    "    caffe_net.blobs['data'].reshape(*(blobs['data'].shape))\n",
    "    if 'im_info' in caffe_net.blobs:\n",
    "        caffe_net.blobs['im_info'].reshape(*(blobs['im_info'].shape))\n",
    "    if not HAS_RPN:\n",
    "        caffe_net.blobs['rois'].reshape(*(blobs['rois'].shape))\n",
    "\n",
    "    # do forward\n",
    "    forward_kwargs = {'data': blobs['data'].astype(np.float32, copy=False)}\n",
    "    if 'im_info' in caffe_net.blobs:\n",
    "        forward_kwargs['im_info'] = blobs['im_info'].astype(np.float32, copy=False)\n",
    "    if not HAS_RPN:\n",
    "        forward_kwargs['rois'] = blobs['rois'].astype(np.float32, copy=False)\n",
    "    blobs_out = caffe_net.forward(**forward_kwargs)\n",
    "\n",
    "\n",
    "    if HAS_RPN:\n",
    "        assert len(im_scales) == 1, \"Only single-image batch implemented\"\n",
    "        rois = net.blobs['rois'].data.copy()\n",
    "        # unscale back to raw image space\n",
    "        boxes = rois[:, 1:5] / im_scales[0]\n",
    "\n",
    "    scores = blobs_out['cls_prob']\n",
    "\n",
    "    # Apply bounding-box regression deltas\n",
    "    box_deltas = blobs_out['bbox_pred']\n",
    "    pred_boxes = bbox_transform_inv(boxes, box_deltas)\n",
    "    pred_boxes = clip_boxes(pred_boxes, im.shape)\n",
    "\n",
    "    if DEDUP_BOXES > 0 and not HAS_RPN:\n",
    "        # Map scores and predictions back to the original set of boxes\n",
    "        scores = scores[inv_index, :]\n",
    "        pred_boxes = pred_boxes[inv_index, :]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
